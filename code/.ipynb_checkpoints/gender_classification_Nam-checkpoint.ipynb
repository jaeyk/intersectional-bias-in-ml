{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender classification\n",
    "\n",
    "Sarah Nam initially wrote this notebook. Jae Yeon Kim reviwed the notebook, edited the markdown, and reproduced, commented on and made substantial changes in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "import collections\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "import nltk as nlp\n",
    "# nltk.download('punkt') You may need to download the dataset\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# ML\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import GaussianNB # Naive-Bayes\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression # Linear models\n",
    "from xgboost import XGBClassifier # Xgboost\n",
    "\n",
    "################### Validation ######################\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import LeavePOut\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "################### Vectorizer ######################\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "################### Model evals #####################\n",
    "from sklearn.metrics import accuracy_score # Accuracy score\n",
    "from sklearn.metrics import balanced_accuracy_score # Balanced accuracy score\n",
    "from sklearn.metrics import cohen_kappa_score # Cohen's Kappa score\n",
    "from sklearn.metrics import precision_score # Precision\n",
    "\n",
    "################### Imbalanced data #####################\n",
    "from sklearn.utils import resample # for resampling\n",
    "\n",
    "# Custom functions\n",
    "from clean_text import clean_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Tweet', 'Type', 'Number of Votes'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tweets = pd.read_csv('/home/jae/intersectional-bias-in-ml/raw_data/tweet.csv')\n",
    "\n",
    "tweets.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99995, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Name columns \n",
    "tweets.columns = ['text', 'label', 'votes']\n",
    "\n",
    "# See dimentions \n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping duplicates to remove the effects of boosting\n",
    "#tweets = tweets.drop_duplicates()\n",
    "#tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text \n",
    "\n",
    "Borrowed the text clean code (`clean_text.py`) from racial classification to make the preprocessing step consistent across different classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>man it would fucking rule if we had a party ...</td>\n",
       "      <td>abusive</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it is time to draw close to him 128591127995 f...</td>\n",
       "      <td>normal</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>if you notice me start to act different or dis...</td>\n",
       "      <td>normal</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>forget unfollowers i believe in growing 7 new ...</td>\n",
       "      <td>normal</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hate being sexually frustrated like i wanna ...</td>\n",
       "      <td>abusive</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    label  votes\n",
       "0    man it would fucking rule if we had a party ...  abusive      4\n",
       "1  it is time to draw close to him 128591127995 f...   normal      4\n",
       "2  if you notice me start to act different or dis...   normal      5\n",
       "3  forget unfollowers i believe in growing 7 new ...   normal      3\n",
       "4    hate being sexually frustrated like i wanna ...  abusive      4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Clean text\n",
    "tweets_clean = tweets.copy()\n",
    "\n",
    "tweets_clean['text'] = clean_tweet(tweets_clean['text'])\n",
    "\n",
    "tweets_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and wrangle training data\n",
    "\n",
    "1. Sarah Nam found an open source Twitter data which was gender labeled (as in, it was written by a person of a certain gender):https://www.kaggle.com/crowdflower/twitter-user-gender-classification\n",
    "2. The dataset includes image, username and other data. She only used the gender label and the sample tweet for training. \n",
    "3. The confidence level of the gender label was also used to sort out which data points would be useful for training our model. I decided that tweets with confidence level greater than .8 were to be used for training. This was because setting confidence threshold to 1 proved to return too few data points for training.\n",
    "\n",
    "* Unfortunately, no clear explanation on what confidence means is provided in the Kaggle webpage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "female     5371\n",
       "male       4658\n",
       "brand      3788\n",
       "unknown     122\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_dat = pd.read_csv(\"/home/jae/intersectional-bias-in-ml/raw_data/gender_classified.csv\", sep=\",\", engine='python')\n",
    "\n",
    "training_data = labeled_dat[labeled_dat['gender:confidence'] > .8][['gender', 'text', 'gender:confidence']]\n",
    "\n",
    "training_data['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting rid of the unknown labeled data\n",
    "training_data = training_data[training_data['gender'] != 'unknown']\n",
    "\n",
    "# Clean text\n",
    "training_data['text'] = clean_tweet(training_data['text']) \n",
    "\n",
    "training_data['text'].isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This dataset included not just male and female tweets, but also tweets by brand twitter accounts and some which were unknown. \n",
    "- Sarah Nam removed tweets for which the gender was unknown, and decided to use two dummy variables to encode the gender. One dummy variable was 1 for male and 0 for non-male (i.e. female or brand). A similar dummy variable was used for females. \n",
    "- Note that the classes are imbalanced in terms of their size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['male', 'female', 'brand'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = training_data.copy()[['text', 'gender']]\n",
    "\n",
    "# Inspect unique values \n",
    "training_data['gender'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create male and female columns \n",
    "training_data['male'] = [1 if i == 'male' else 0 for i in training_data['gender'].values]\n",
    "training_data['female'] = [1 if i == 'female' else 0 for i in training_data['gender'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    9159\n",
       "1    4658\n",
       "Name: male, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the class balance \n",
    "\n",
    "## Male\n",
    "training_data['male'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8446\n",
       "1    5371\n",
       "Name: female, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## Female\n",
    "training_data['female'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling \n",
    "\n",
    "To fix the imbalance problem, Jae Kim randomly oversampled the minority class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function for upsample. Jae Kim adapted some code from here: https://elitedatascience.com/imbalanced-classes \n",
    "\n",
    "def upsample(data, condition): \n",
    "\n",
    "    df_majority = data[data[condition] == 0]\n",
    "    df_minority = data[data[condition] == 1]\n",
    "    \n",
    "    # Upsample (oversample) minority class \n",
    "    \n",
    "    df_minority_upsampled = resample(df_minority, \n",
    "                                 replace = True,     # sample with replacement\n",
    "                                 n_samples = 8000,    # to match majority class\n",
    "                                 random_state = 1234) # reproducible results\n",
    "    \n",
    "    # Combine majority class with upsampled minority class\n",
    "    data = pd.concat([df_majority, df_minority_upsampled])\n",
    "    \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Feature extraction (bag-of-words model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Vectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(strip_accents='ascii', \n",
    "                             max_features = 5000, # 5,000 is large enough\n",
    "                             min_df = 1, # minimum frequency 1\n",
    "                             ngram_range = (1,2), # ngram \n",
    "                             binary = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn text into document-term matrix\n",
    "\n",
    "def dtm_train(data, condition):\n",
    "\n",
    "    ############################### Upsampling ################################\n",
    "\n",
    "    data = upsample(data, condition)\n",
    "    \n",
    "    ############################### DOCUMENT-TERM MATRIX ################################\n",
    "    \n",
    "    # BOW model \n",
    "    \n",
    "    features = vectorizer.fit_transform(data['text']).todense() # Turn into a sparse matrix    \n",
    "\n",
    "    # Response variable\n",
    "    \n",
    "    response = data[condition].values # values \n",
    "\n",
    "    ############################### STRATIFIED RANDOM SAMPLING ################################\n",
    "    \n",
    "    # Split into training and testing sets \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, response, \n",
    "                                                        test_size = 0.2, # training = 80%, test = 20%\n",
    "                                                        random_state = 1234) \n",
    "    \n",
    "    return(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Male DTM\n",
    "\n",
    "male_dtm = dtm_train(training_data, 'male')\n",
    "\n",
    "# Female DTM\n",
    "\n",
    "female_dtm = dtm_train(training_data, 'female')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifiers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for various ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso\n",
    "\n",
    "def fit_logistic_regression(X_train, y_train):\n",
    "    model = LogisticRegression(fit_intercept = True,\n",
    "                               penalty = 'l1', # Lasso \n",
    "                               solver = 'liblinear') # for small datasets\n",
    "    # sage solver is faster but doesn't coverge in this case\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# Naive-Bayes \n",
    "\n",
    "def fit_bayes(X_train, y_train):\n",
    "    model = GaussianNB()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# Xgboost\n",
    "\n",
    "def fit_xgboost(X_train, y_train):\n",
    "    model = XGBClassifier(random_state = 42,\n",
    "                         seed = 2, \n",
    "                         colsample_bytree = 0.6,\n",
    "                         subsample = 0.7)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for evaluating ML models (accuracy and balanced accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, X_train, y_train, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "#   print(\"Accuracy:\", accuracy, \"\\n\"\n",
    "#          \"Balanced accuracy:\", balanced_accuracy)\n",
    "    return(accuracy, balanced_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_models(data):\n",
    "    # Lasso\n",
    "    lasso = fit_logistic_regression(data[0], data[1])\n",
    "    # Naive-Bayes\n",
    "    bayes = fit_bayes(data[0], data[1])\n",
    "    # Xgboost\n",
    "    xgboost = fit_xgboost(data[0], data[1])\n",
    "    \n",
    "    return(lasso, bayes, xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "male_fit = fit_models(male_dtm)\n",
    "\n",
    "female_fit = fit_models(female_dtm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluations \n",
    "\n",
    "### Function for testing multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_models(models, data):\n",
    "    lasso = test_model(models[0], data[0], data[1], data[2], data[3]) \n",
    "    bayes = test_model(models[1], data[0], data[1], data[2], data[3])\n",
    "    xgboost = test_model(models[2], data[0], data[1], data[2], data[3])\n",
    "    return(lasso, bayes, xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate multiple models for each data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_models = test_models(male_fit, male_dtm)\n",
    "\n",
    "female_models = test_models(female_fit, female_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for putting the model evaluations into a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_table(data):\n",
    "    table = pd.DataFrame(list(data), columns= ['Accuracy','Balanced Accuracy'])\n",
    "    table.insert(loc = 0, column = 'Models', value = ['Lasso', 'Bayes', 'XGBoost'])\n",
    "    return(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.701923</td>\n",
       "      <td>0.700363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bayes</td>\n",
       "      <td>0.618881</td>\n",
       "      <td>0.632854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.667249</td>\n",
       "      <td>0.660138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Models  Accuracy  Balanced Accuracy\n",
       "0    Lasso  0.701923           0.700363\n",
       "1    Bayes  0.618881           0.632854\n",
       "2  XGBoost  0.667249           0.660138"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "eval_table(male_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.753191</td>\n",
       "      <td>0.753121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bayes</td>\n",
       "      <td>0.681459</td>\n",
       "      <td>0.683990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.713070</td>\n",
       "      <td>0.712317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Models  Accuracy  Balanced Accuracy\n",
       "0    Lasso  0.753191           0.753121\n",
       "1    Bayes  0.681459           0.683990\n",
       "2  XGBoost  0.713070           0.712317"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "eval_table(female_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for predicting the unlabeled data (tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(text, model):   \n",
    "      \n",
    "    # BOW model \n",
    "    \n",
    "    features = vectorizer.fit_transform(text).todense()\n",
    "    \n",
    "    # Prediction\n",
    "    \n",
    "    preds = model.predict(features)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the function to the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_predicted = predict_text(tweets_clean['text'], male_fit[0])\n",
    "female_predicted = predict_text(tweets_clean['text'], female_fit[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data quality check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The only data cleaning that was done was to lower case everything.\n",
    "# Verifying there are no null entries in the tweet text.\n",
    "tweets_clean['text'].isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_clean['male'] = male_predicted\n",
    "tweets_clean['female'] = female_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    53068\n",
       "1    46927\n",
       "Name: male, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_clean['male'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    60492\n",
       "0    39503\n",
       "Name: female, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_clean['female'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>votes</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>man it would fucking rule if we had a party ...</td>\n",
       "      <td>abusive</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it is time to draw close to him 128591127995 f...</td>\n",
       "      <td>normal</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>if you notice me start to act different or dis...</td>\n",
       "      <td>normal</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>forget unfollowers i believe in growing 7 new ...</td>\n",
       "      <td>normal</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hate being sexually frustrated like i wanna ...</td>\n",
       "      <td>abusive</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    label  votes  male  \\\n",
       "0    man it would fucking rule if we had a party ...  abusive      4     1   \n",
       "1  it is time to draw close to him 128591127995 f...   normal      4     0   \n",
       "2  if you notice me start to act different or dis...   normal      5     1   \n",
       "3  forget unfollowers i believe in growing 7 new ...   normal      3     1   \n",
       "4    hate being sexually frustrated like i wanna ...  abusive      4     1   \n",
       "\n",
       "   female  \n",
       "0       0  \n",
       "1       0  \n",
       "2       1  \n",
       "3       0  \n",
       "4       1  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the predicted values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_clean.to_csv(\"/home/jae/intersectional-bias-in-ml/processed_data/gender_predictions.csv\", sep=',', encoding='utf-8', \n",
    "                    header=[\"text\", \"label\", \"votes\", \"male\", \"female\"], index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
