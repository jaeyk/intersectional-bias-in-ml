{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender classification\n",
    "\n",
    "Sarah Nam initially wrote this notebook. Jae Yeon Kim reviwed the notebook, edited the markdown, and reproduced, commented on and made substantial changes in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/home/jae/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "import collections\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "import nltk as nlp\n",
    "# nltk.download('punkt') You may need to download the dataset\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# ML\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import GaussianNB # Naive-Bayes\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression # Linear models\n",
    "from xgboost import XGBClassifier # Xgboost\n",
    "\n",
    "################### Validation ######################\n",
    "from sklearn.model_selection import train_test_split, KFold, LeaveOneOut, LeavePOut, ShuffleSplit, StratifiedKFold\n",
    "\n",
    "################### Vectorizer ######################\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "################### Model evals #####################\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, cohen_kappa_score, precision_score, recall_score\n",
    "\n",
    "################### Imbalanced data #####################\n",
    "from sklearn.utils import resample # for resampling\n",
    "\n",
    "# Custom functions\n",
    "from clean_text import clean_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([0, 1, 2], dtype='int64')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tweets = pd.read_csv('/home/jae/intersectional-bias-in-ml/raw_data/hatespeech_text_label_vote_RESTRICTED_100K.csv', sep='\\t', header=None)\n",
    "\n",
    "tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99996, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Name columns \n",
    "tweets.columns = ['text', 'label', 'votes']\n",
    "\n",
    "# See dimentions \n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping duplicates to remove the effects of boosting\n",
    "#tweets = tweets.drop_duplicates()\n",
    "#tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text \n",
    "\n",
    "Borrowed the text clean code (`clean_text.py`) from racial classification to make the preprocessing step consistent across different classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beats by dr dre urbeats wired inear headphones...</td>\n",
       "      <td>spam</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>man it would fucking rule if we had a party ...</td>\n",
       "      <td>abusive</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it is time to draw close to him 128591127995 f...</td>\n",
       "      <td>normal</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if you notice me start to act different or dis...</td>\n",
       "      <td>normal</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>forget unfollowers i believe in growing 7 new ...</td>\n",
       "      <td>normal</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    label  votes\n",
       "0  beats by dr dre urbeats wired inear headphones...     spam      4\n",
       "1    man it would fucking rule if we had a party ...  abusive      4\n",
       "2  it is time to draw close to him 128591127995 f...   normal      4\n",
       "3  if you notice me start to act different or dis...   normal      5\n",
       "4  forget unfollowers i believe in growing 7 new ...   normal      3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Clean text\n",
    "tweets_clean = tweets.copy()\n",
    "\n",
    "tweets_clean['text'] = clean_tweet(tweets_clean['text'])\n",
    "\n",
    "tweets_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and wrangle training data\n",
    "\n",
    "1. Sarah Nam found an open source Twitter data which was gender labeled (as in, it was written by a person of a certain gender):https://www.kaggle.com/crowdflower/twitter-user-gender-classification\n",
    "2. The dataset includes image, username and other data. She only used the gender label and the sample tweet for training. \n",
    "3. The confidence level of the gender label was also used to sort out which data points would be useful for training our model. I decided that tweets with confidence level greater than .8 were to be used for training. This was because setting confidence threshold to 1 proved to return too few data points for training.\n",
    "\n",
    "* Unfortunately, no clear explanation on what confidence means is provided in the Kaggle webpage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "female     5371\n",
       "male       4658\n",
       "brand      3788\n",
       "unknown     122\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_dat = pd.read_csv(\"/home/jae/intersectional-bias-in-ml/raw_data/gender_classified.csv\", sep=\",\", engine='python')\n",
    "\n",
    "training_data = labeled_dat[labeled_dat['gender:confidence'] > .8][['gender', 'text', 'gender:confidence']]\n",
    "\n",
    "training_data['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting rid of the unknown labeled data\n",
    "training_data = training_data[training_data['gender'] != 'unknown']\n",
    "\n",
    "# Clean text\n",
    "training_data['text'] = clean_tweet(training_data['text']) \n",
    "\n",
    "training_data['text'].isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This dataset included not just male and female tweets, but also tweets by brand twitter accounts and some which were unknown. \n",
    "- Sarah Nam removed tweets for which the gender was unknown, and decided to use two dummy variables to encode the gender. One dummy variable was 1 for male and 0 for non-male (i.e. female or brand). A similar dummy variable was used for females. \n",
    "- Note that the classes are imbalanced in terms of their size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['male', 'female', 'brand'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = training_data.copy()[['text', 'gender']]\n",
    "\n",
    "# Inspect unique values \n",
    "training_data['gender'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create male and female columns \n",
    "training_data['male'] = [1 if i == 'male' else 0 for i in training_data['gender'].values]\n",
    "training_data['female'] = [1 if i == 'female' else 0 for i in training_data['gender'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    9159\n",
       "1    4658\n",
       "Name: male, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Check the class balance \n",
    "\n",
    "## Male\n",
    "training_data['male'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8446\n",
       "1    5371\n",
       "Name: female, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Female\n",
    "training_data['female'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix the imbalance problem, Jae Kim randomly oversampled the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function for upsample. Jae Kim adapted some code from here: https://elitedatascience.com/imbalanced-classes \n",
    "\n",
    "def upsample(data, condition): \n",
    "\n",
    "    df_majority = data[data[condition] == 0]\n",
    "    df_minority = data[data[condition] == 1]\n",
    "    \n",
    "    # Upsample (oversample) minority class \n",
    "    \n",
    "    df_minority_upsampled = resample(df_minority, \n",
    "                                 replace = True,     # sample with replacement\n",
    "                                 n_samples = 9000,    # to match majority class\n",
    "                                 random_state = 1234) # reproducible results\n",
    "    \n",
    "    # Combine majority class with upsampled minority class\n",
    "    data = pd.concat([df_majority, df_minority_upsampled])\n",
    "    \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Feature extraction (bag-of-words model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = LancasterStemmer()\n",
    "\n",
    "def token(text):\n",
    "    txt = nlp.word_tokenize(text.lower())\n",
    "    return [st.stem(word) for word in txt]\n",
    "\n",
    "\n",
    "# Vectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 4000, # 4,000 is large enough\n",
    "                             min_df = 1, # minimum frequency 1\n",
    "                             ngram_range = (1,2), # ngram \n",
    "                             tokenizer = token,\n",
    "                             analyzer=u'word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn text into document-term matrix\n",
    "\n",
    "def dtm_train(data, condition):\n",
    "    \n",
    "    ############################### Upsampling ################################\n",
    "\n",
    "    data = upsample(data, condition)\n",
    "    \n",
    "    ############################### DOCUMENT-TERM MATRIX ################################\n",
    "    \n",
    "    # BOW model \n",
    "    \n",
    "    features = vectorizer.fit_transform(data['text']).todense()\n",
    "    \n",
    "    # Response variable\n",
    "    \n",
    "    response = data[condition].values # values \n",
    "\n",
    "    ############################### STRATIFIED RANDOM SAMPLING ################################\n",
    "    \n",
    "    # Split into training and testing sets \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, response, \n",
    "                                                        test_size = 0.2, # training = 80%, test = 20%\n",
    "                                                        random_state = 1234) \n",
    "    \n",
    "    return(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Male DTM\n",
    "\n",
    "male_dtm = dtm_train(training_data, 'male')\n",
    "\n",
    "# Female DTM\n",
    "\n",
    "female_dtm = dtm_train(training_data, 'female')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifiers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for various ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso\n",
    "\n",
    "def fit_logistic_regression(X_train, y_train):\n",
    "    model = LogisticRegression(penalty = 'l1', # Lasso \n",
    "                               solver = 'liblinear') # for small datasets\n",
    "    # sage solver is faster but doesn't coverge in this case\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# Naive-Bayes \n",
    "\n",
    "def fit_bayes(X_train, y_train):\n",
    "    model = GaussianNB()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# Xgboost\n",
    "\n",
    "def fit_xgboost(X_train, y_train):\n",
    "    model = XGBClassifier(random_state = 42,\n",
    "                         seed = 2, \n",
    "                         colsample_bytree = 0.6,\n",
    "                         subsample = 0.7)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for evaluating ML models (accuracy and balanced accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, X_train, y_train, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    return(accuracy, precision, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_models(data):\n",
    "    # Lasso\n",
    "    lasso = fit_logistic_regression(data[0], data[1])\n",
    "    # Naive-Bayes\n",
    "    bayes = fit_bayes(data[0], data[1])\n",
    "    # Xgboost\n",
    "    xgboost = fit_xgboost(data[0], data[1])\n",
    "    \n",
    "    return(lasso, bayes, xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "male_fit = fit_models(male_dtm)\n",
    "\n",
    "female_fit = fit_models(female_dtm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluations \n",
    "\n",
    "### Function for testing multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_models(models, data):\n",
    "    lasso = test_model(models[0], data[0], data[1], data[2], data[3]) \n",
    "    bayes = test_model(models[1], data[0], data[1], data[2], data[3])\n",
    "    xgboost = test_model(models[2], data[0], data[1], data[2], data[3])\n",
    "    return(lasso, bayes, xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate multiple models for each data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_models = test_models(male_fit, male_dtm)\n",
    "\n",
    "female_models = test_models(female_fit, female_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for putting the model evaluations into a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation function \n",
    "\n",
    "def eval_table(data):\n",
    "    # Turn it into a data frame \n",
    "    table = pd.DataFrame(list(data), columns= ['Accuracy','Precision','Recall']) \n",
    "    # Add column names \n",
    "    table.insert(loc = 0, column = 'Models', value = ['Lasso', 'Bayes', 'XGBoost'])\n",
    "    # Round to 2 decimals \n",
    "    table = round(table, 2)\n",
    "    return(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to each model outcomes and add gender columns \n",
    "\n",
    "eval_male_models = eval_table(male_models)\n",
    "eval_male_models['Label'] = 'Male'\n",
    "\n",
    "eval_feamle_models = eval_table(female_models)\n",
    "eval_feamle_models['Label'] = 'Female'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.73</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bayes</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.86</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.77</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bayes</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.84</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.71</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Models  Accuracy  Precision  Recall   Label\n",
       "0    Lasso      0.69       0.68    0.73    Male\n",
       "1    Bayes      0.62       0.58    0.86    Male\n",
       "2  XGBoost      0.65       0.65    0.65    Male\n",
       "0    Lasso      0.73       0.73    0.77  Female\n",
       "1    Bayes      0.69       0.65    0.84  Female\n",
       "2  XGBoost      0.71       0.72    0.71  Female"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the two data frames \n",
    "\n",
    "eval_gender_models = pd.concat([eval_male_models, eval_feamle_models])\n",
    "\n",
    "eval_gender_models \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the data \n",
    "\n",
    "eval_gender_models.to_csv(\"/home/jae/intersectional-bias-in-ml/processed_data/eval_gender_models.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for predicting the unlabeled data (tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(text, model):   \n",
    "      \n",
    "    # BOW model \n",
    "    \n",
    "    features = vectorizer.fit_transform(text).todense()\n",
    "    \n",
    "    # Prediction\n",
    "    \n",
    "    preds = model.predict(features)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the function to the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_predicted = predict_text(tweets_clean['text'], male_fit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_predicted = predict_text(tweets_clean['text'], female_fit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_results = []\n",
    "\n",
    "for i in np.arange(len(male_predicted)):\n",
    "    if male_predicted[i] == 1 and female_predicted[i] == 0:\n",
    "        fin_results.append('male')\n",
    "    elif male_predicted[i] == 0 and female_predicted[i] == 1:\n",
    "        fin_results.append('female')\n",
    "    elif male_predicted[i] == 1 and female_predicted[i] == 1:\n",
    "        fin_results.append('both')\n",
    "    else:\n",
    "        fin_results.append('neither')\n",
    "\n",
    "        \n",
    "tweets_clean['gender'] = fin_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data quality check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The only data cleaning that was done was to lower case everything.\n",
    "# Verifying there are no null entries in the tweet text.\n",
    "tweets_clean['text'].isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_clean['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_clean['male'] = male_predicted\n",
    "tweets_clean['female'] = female_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_clean['male'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_clean['female'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the predicted values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_clean.to_csv(\"/home/jae/intersectional-bias-in-ml/processed_data/gender_predictions.csv\", sep=',', encoding='utf-8', \n",
    "          #          header=[\"text\", \"label\", \"votes\", \"gender\", \"male\", \"female\"], index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
